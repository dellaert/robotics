{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02-DuckieBot2.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dhzCZ0I5GY3q"
      },
      "source": [
        "# DuckieBot II\n",
        "* Robot: DuckieBot\n",
        "* Sensor: wheel encoder\n",
        "* Think: RL\n",
        "* Act: DD\n",
        "* Topics needed: RL graphs & KF\n",
        "* Factor graph concepts: \n"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Discrete States and Actions"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Classical Planning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4qUYcGjLGTaQ"
      },
      "source": [
        "## MDP\n",
        "A Markov decision process (MDP) has states $x$ and actions $a$..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "K5BtAxD9G2Au"
      },
      "source": [
        "## Reinforcement Learning "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yY0_0M00IXYt"
      },
      "source": [
        "## Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5SjqkwjRIUA4"
      },
      "source": [
        "## Policy Gradient"
      ]
    }
  ]
}